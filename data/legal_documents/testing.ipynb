{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/infres/apereira-22/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/infres/apereira-22/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(csv_path):\n",
    "    \"\"\"\n",
    "    Load the dataset from a CSV file.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    return df\n",
    "\n",
    "def perform_eda(df):\n",
    "    \"\"\"\n",
    "    Perform basic EDA on the dataset.\n",
    "    \"\"\"\n",
    "    print(\"First 5 records:\")\n",
    "    print(df.head())\n",
    "\n",
    "    print(\"\\nDataset Info:\")\n",
    "    print(df.info())\n",
    "\n",
    "    print(\"\\nMissing Values:\")\n",
    "    print(df.isnull().sum())\n",
    "\n",
    "    print(\"\\nClass Distribution:\")\n",
    "    print(df['case_outcome'].value_counts())\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean the input text by:\n",
    "    - Lowercasing\n",
    "    - Removing special characters and digits\n",
    "    - Removing stopwords\n",
    "    - Lemmatizing\n",
    "    \"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    # Join back to string\n",
    "    cleaned_text = ' '.join(tokens)\n",
    "    return cleaned_text\n",
    "\n",
    "def preprocess_texts(df):\n",
    "    \"\"\"\n",
    "    Apply text cleaning to 'case_title' and 'case_text'.\n",
    "    \"\"\"\n",
    "    df['cleaned_title'] = df['case_title'].apply(lambda x: clean_text(str(x)))\n",
    "    df['cleaned_text'] = df['case_text'].apply(lambda x: clean_text(str(x)))\n",
    "    return df\n",
    "\n",
    "def split_into_chunks(text, max_length=512):\n",
    "    \"\"\"\n",
    "    Split text into chunks of maximum 'max_length' words.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chunks = [' '.join(words[i:i + max_length]) for i in range(0, len(words), max_length)]\n",
    "    return chunks\n",
    "\n",
    "def split_texts(df, max_length=512):\n",
    "    \"\"\"\n",
    "    Apply text splitting to 'cleaned_text'.\n",
    "    Each case can have multiple chunks.\n",
    "    \"\"\"\n",
    "    df['text_chunks'] = df['cleaned_text'].apply(lambda x: split_into_chunks(x, max_length))\n",
    "    return df\n",
    "\n",
    "def generate_embeddings(df, model_name='sentence-transformers/all-MiniLM-L6-v2'):\n",
    "    \"\"\"\n",
    "    Generate embeddings for each text chunk.\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "    all_chunks = df['text_chunks'].explode().tolist()\n",
    "    \n",
    "    print(\"Generating embeddings...\")\n",
    "    embeddings = model.encode(all_chunks, show_progress_bar=True, convert_to_numpy=True)\n",
    "    \n",
    "    return all_chunks, embeddings\n",
    "\n",
    "def build_faiss_index(embeddings, index_path):\n",
    "    \"\"\"\n",
    "    Build and save a FAISS index from embeddings.\n",
    "    \"\"\"\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)  # Using L2 distance; consider IndexHNSWFlat for larger datasets\n",
    "    index.add(embeddings)\n",
    "    faiss.write_index(index, index_path)\n",
    "    print(f\"FAISS index built and saved to {index_path}\")\n",
    "    return index\n",
    "\n",
    "def save_chunks(all_chunks, doc_ids, save_path='data/processed_data/documents.pkl'):\n",
    "    \"\"\"\n",
    "    Save the text chunks and their corresponding document IDs.\n",
    "    \"\"\"\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump({'chunks': all_chunks, 'doc_ids': doc_ids}, f)\n",
    "    print(f\"Documents saved to {save_path}\")\n",
    "\n",
    "def save_preprocessed_dataframe(df, save_path):\n",
    "    \"\"\"\n",
    "    Save the preprocessed dataframe to a CSV file.\n",
    "    \"\"\"\n",
    "    df.to_csv(save_path, index=False)\n",
    "    print(f\"Preprocessed data saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"raw_legal_text_classification.csv\"\n",
    "df = load_dataset(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 records:\n",
      "  case_id case_outcome                                         case_title  \\\n",
      "0   Case1        cited  Alpine Hardwood (Aust) Pty Ltd v Hardys Pty Lt...   \n",
      "1   Case2        cited  Black v Lipovac [1998] FCA 699 ; (1998) 217 AL...   \n",
      "2   Case3        cited  Colgate Palmolive Co v Cussons Pty Ltd (1993) ...   \n",
      "3   Case4        cited  Dais Studio Pty Ltd v Bullett Creative Pty Ltd...   \n",
      "4   Case5        cited  Dr Martens Australia Pty Ltd v Figgins Holding...   \n",
      "\n",
      "                                           case_text  \n",
      "0  Ordinarily that discretion will be exercised s...  \n",
      "1  The general principles governing the exercise ...  \n",
      "2  Ordinarily that discretion will be exercised s...  \n",
      "3  The general principles governing the exercise ...  \n",
      "4  The preceding general principles inform the ex...  \n",
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24985 entries, 0 to 24984\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   case_id       24985 non-null  object\n",
      " 1   case_outcome  24985 non-null  object\n",
      " 2   case_title    24985 non-null  object\n",
      " 3   case_text     24809 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 780.9+ KB\n",
      "None\n",
      "\n",
      "Missing Values:\n",
      "case_id           0\n",
      "case_outcome      0\n",
      "case_title        0\n",
      "case_text       176\n",
      "dtype: int64\n",
      "\n",
      "Class Distribution:\n",
      "case_outcome\n",
      "cited            12219\n",
      "referred to       4384\n",
      "applied           2448\n",
      "followed          2256\n",
      "considered        1712\n",
      "discussed         1024\n",
      "distinguished      608\n",
      "related            113\n",
      "affirmed           113\n",
      "approved           108\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "perform_eda(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 records:\n",
      "  case_id case_outcome                                         case_title  \\\n",
      "0   Case1        cited  Alpine Hardwood (Aust) Pty Ltd v Hardys Pty Lt...   \n",
      "1   Case2        cited  Black v Lipovac [1998] FCA 699 ; (1998) 217 AL...   \n",
      "2   Case3        cited  Colgate Palmolive Co v Cussons Pty Ltd (1993) ...   \n",
      "3   Case4        cited  Dais Studio Pty Ltd v Bullett Creative Pty Ltd...   \n",
      "4   Case5        cited  Dr Martens Australia Pty Ltd v Figgins Holding...   \n",
      "\n",
      "                                           case_text  \n",
      "0  Ordinarily that discretion will be exercised s...  \n",
      "1  The general principles governing the exercise ...  \n",
      "2  Ordinarily that discretion will be exercised s...  \n",
      "3  The general principles governing the exercise ...  \n",
      "4  The preceding general principles inform the ex...  \n",
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 24809 entries, 0 to 24984\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   case_id       24809 non-null  object\n",
      " 1   case_outcome  24809 non-null  object\n",
      " 2   case_title    24809 non-null  object\n",
      " 3   case_text     24809 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 969.1+ KB\n",
      "None\n",
      "\n",
      "Missing Values:\n",
      "case_id         0\n",
      "case_outcome    0\n",
      "case_title      0\n",
      "case_text       0\n",
      "dtype: int64\n",
      "\n",
      "Class Distribution:\n",
      "case_outcome\n",
      "cited            12110\n",
      "referred to       4363\n",
      "applied           2438\n",
      "followed          2252\n",
      "considered        1699\n",
      "discussed         1018\n",
      "distinguished      603\n",
      "related            112\n",
      "approved           108\n",
      "affirmed           106\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna(subset=['case_text'])\n",
    "perform_eda(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 records:\n",
      "   case_id case_outcome                                         case_title  \\\n",
      "0        1        cited  Alpine Hardwood (Aust) Pty Ltd v Hardys Pty Lt...   \n",
      "1        2        cited  Black v Lipovac [1998] FCA 699 ; (1998) 217 AL...   \n",
      "2        3        cited  Colgate Palmolive Co v Cussons Pty Ltd (1993) ...   \n",
      "3        4        cited  Dais Studio Pty Ltd v Bullett Creative Pty Ltd...   \n",
      "4        5        cited  Dr Martens Australia Pty Ltd v Figgins Holding...   \n",
      "\n",
      "                                           case_text  \n",
      "0  Ordinarily that discretion will be exercised s...  \n",
      "1  The general principles governing the exercise ...  \n",
      "2  Ordinarily that discretion will be exercised s...  \n",
      "3  The general principles governing the exercise ...  \n",
      "4  The preceding general principles inform the ex...  \n",
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 24809 entries, 0 to 24984\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   case_id       24809 non-null  int64 \n",
      " 1   case_outcome  24809 non-null  object\n",
      " 2   case_title    24809 non-null  object\n",
      " 3   case_text     24809 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 969.1+ KB\n",
      "None\n",
      "\n",
      "Missing Values:\n",
      "case_id         0\n",
      "case_outcome    0\n",
      "case_title      0\n",
      "case_text       0\n",
      "dtype: int64\n",
      "\n",
      "Class Distribution:\n",
      "case_outcome\n",
      "cited            12110\n",
      "referred to       4363\n",
      "applied           2438\n",
      "followed          2252\n",
      "considered        1699\n",
      "discussed         1018\n",
      "distinguished      603\n",
      "related            112\n",
      "approved           108\n",
      "affirmed           106\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df['case_id'] = df['case_id'].str.replace('Case', '').astype(int)\n",
    "df['case_outcome'] = df['case_outcome'].astype(str)\n",
    "df['case_title'] = df['case_title'].astype(str)\n",
    "df['case_text'] = df['case_text'].astype(str)\n",
    "perform_eda(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocess_texts(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    24809.000000\n",
       "mean       222.178000\n",
       "std        558.954086\n",
       "min         11.000000\n",
       "25%         74.000000\n",
       "50%        120.000000\n",
       "75%        212.000000\n",
       "max      10998.000000\n",
       "Name: cleaned_text, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned_text'].apply(lambda x: len(x.split())).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = split_texts(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92b8d02439a64859a99be5b1320e8cc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/888 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_chunks, embeddings = generate_embeddings(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chunks, embeddings = generate_embeddings(df)\n",
    "with open('temp_all_chunks.pkl', 'wb') as f:\n",
    "    pickle.dump(all_chunks, f)\n",
    "np.save('temp_embeddings.npy', embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chunks = pickle.load(open('temp_all_chunks.pkl', 'rb'))\n",
    "embeddings = np.load('temp_embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28391, (28391, 384))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_chunks), embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built and saved to faiss_index.index\n"
     ]
    }
   ],
   "source": [
    "index = build_faiss_index(embeddings, 'faiss_index.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents saved to documents.pkl\n"
     ]
    }
   ],
   "source": [
    "doc_ids = df.loc[df.index.repeat(df['text_chunks'].str.len()), 'case_id'].tolist()\n",
    "save_chunks(all_chunks, doc_ids, save_path=\"documents.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28391"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data saved to preprocessed_dataframe.csv\n"
     ]
    }
   ],
   "source": [
    "save_preprocessed_dataframe(df, 'preprocessed_dataframe.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
